{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/cpu/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import scipy\n",
    "import shutil\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.layers import Dense, Lambda, Input, Flatten, BatchNormalization\n",
    "from keras.models import load_model, Model, model_from_json\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.preprocessing import image\n",
    "from keras import backend as K\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "INPUTS_FNAME   = \"lfw_datasets_and_models.zip\"\n",
    "\n",
    "PATH_INPUTS_FNAME     = \"./lfw_datasets_and_models.zip\" \n",
    "PATH_INPUTS           = \"./test/lfw_datasets_and_models\"\n",
    "\n",
    "PATH_DATASET_BASE     = PATH_INPUTS + \"/datasets\"\n",
    "\n",
    "PATH_DATASET_BASE_MASKED   = PATH_DATASET_BASE + \"/masked\"\n",
    "PATH_DATASET_BASE_UNMASKED = PATH_DATASET_BASE + \"/unmasked\"\n",
    "\n",
    "PATH_DATASET_MASKED_TRAIN = PATH_DATASET_BASE_MASKED + \"/train/\"\n",
    "PATH_DATASET_MASKED_VAL   = PATH_DATASET_BASE_MASKED + \"/validation/\"\n",
    "PATH_DATASET_MASKED_TEST  = PATH_DATASET_BASE_MASKED + \"/test/\"\n",
    "\n",
    "PATH_DATASET_UNMASKED_TRAIN = PATH_DATASET_BASE_UNMASKED + \"/train/\"\n",
    "PATH_DATASET_UNMASKED_VAL   = PATH_DATASET_BASE_UNMASKED + \"/validation/\"\n",
    "PATH_DATASET_UNMASKED_TEST  = PATH_DATASET_BASE_UNMASKED + \"/test/\"\n",
    "\n",
    "PATH_TRAIN_RESNET = PATH_INPUTS + '/models/resnet/retrained/'\n",
    "PATH_TRAIN_FACENET = PATH_INPUTS + '/models/facenet/retrained/'\n",
    "PATH_TRAIN_DATASET = PATH_INPUTS + '/training'\n",
    "PATH_TRAIN_ANCHOR_DATASET = PATH_TRAIN_DATASET + '/anchor/'\n",
    "PATH_TRAIN_POSITIVE_DATASET = PATH_TRAIN_DATASET + '/positive/'\n",
    "\n",
    "PATH_FACENET_KERAS_H5 = PATH_INPUTS + \"/models/facenet/pretrained/model/facenet_keras.h5\"\n",
    "\n",
    "TRIPLET_LOSS_MARGIN = 2\n",
    "\n",
    "IMAGE_INPUT_SIZE = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_zipfile(filename: str, extract_dirname: str, extract_path: str):\n",
    "    # Extract the inputs from the zip file.\n",
    "    if (not os.path.isdir(extract_dirname)):\n",
    "        print(\"[INFO] Extracting from '{}' to '../'...\".format(filename), end=\" \")\n",
    "        with zipfile.ZipFile(filename, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "        print(\"DONE.\")\n",
    "    else:\n",
    "        print(\"[INFO] Directory '{}' exists.\".format(extract_dirname))\n",
    "\n",
    "#extract_zipfile(PATH_INPUTS_FNAME, PATH_INPUTS, \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [Retraing]\n",
    "def triplet_loss(inputs, dist='euclidean', margin='maxplus'):\n",
    "    anchor, positive, negative = inputs\n",
    "    positive_distance = K.square(anchor - positive)\n",
    "    negative_distance = K.square(anchor - negative)\n",
    "    if dist == 'euclidean':\n",
    "        positive_distance = K.sqrt(K.sum(positive_distance, axis=-1, keepdims=True))\n",
    "        negative_distance = K.sqrt(K.sum(negative_distance, axis=-1, keepdims=True))\n",
    "    elif dist == 'sqeuclidean':\n",
    "        positive_distance = K.sum(positive_distance, axis=-1, keepdims=True)\n",
    "        negative_distance = K.sum(negative_distance, axis=-1, keepdims=True)\n",
    "    loss = positive_distance - negative_distance\n",
    "    if margin == 'maxplus':\n",
    "        loss = K.maximum(0.0, TRIPLET_LOSS_MARGIN + loss)\n",
    "    elif margin == 'softplus':\n",
    "        loss = K.log(1 + K.exp(loss))\n",
    "        \n",
    "    returned_loss = K.mean(loss)\n",
    "    return returned_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [Retraining]\n",
    "# Used when compiling the siamese network\n",
    "def identity_loss(y_true, y_pred):\n",
    "    return K.mean(y_pred - 0 * y_true)  # This is actually just returning y_pred bcs\n",
    "                                        # K.mean has already been called in the triplet_loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# [Retraining]\n",
    "def get_siamese_from_facenet():\n",
    "    model = load_model(PATH_FACENET_KERAS_H5)\n",
    "    print(len(model.layers))\n",
    "    model.layers.pop()\n",
    "    model.layers.pop()\n",
    "    #model.summary()\n",
    "\n",
    "    layers_count = len(model.layers)\n",
    "    for i in range(layers_count - 10):\n",
    "        layer = model.layers[i]\n",
    "        if i < layers_count//2:\n",
    "            layer.trainable = False # Freeze first half of layers.\n",
    "        elif layer.trainable and not layer.name.endswith(\"BatchNorm\"):\n",
    "            layer.trainable = False # Leave all BatchNorm layers to retrain.\n",
    "\n",
    "    last_layer = model.layers[layers_count - 1]\n",
    "    assert last_layer.trainable == True # Ensure last layer to retrain.\n",
    "\n",
    "    # Define the siamese facenet network\n",
    "    image_shape = (IMAGE_INPUT_SIZE, IMAGE_INPUT_SIZE, 3)\n",
    "\n",
    "    model_out = last_layer.output\n",
    "    model_out = Dense(128, activation = 'relu',  name = 'model_out')(model_out)\n",
    "    model_out = BatchNormalization(axis = 1, epsilon=0.00001, name = 'BatchNorm_last')(model_out)\n",
    "    model_out = Lambda(lambda  x: K.l2_normalize(x, axis = 1))(model_out)\n",
    "\n",
    "    new_model = Model(inputs=model.input, outputs=model_out)\n",
    "\n",
    "    anchor_input = Input(shape=image_shape, name ='anchor_input')\n",
    "    pos_input = Input(shape=image_shape, name ='pos_input')\n",
    "    neg_input = Input(shape=image_shape, name ='neg_input')\n",
    "\n",
    "    encoding_anchor = new_model(anchor_input)\n",
    "    encoding_pos = new_model(pos_input)\n",
    "    encoding_neg = new_model(neg_input)\n",
    "\n",
    "    loss = Lambda(triplet_loss)([encoding_anchor, encoding_pos, encoding_neg])\n",
    "\n",
    "    siamese_facenet = Model(inputs  = [anchor_input, pos_input, neg_input], outputs = loss)\n",
    "    siamese_facenet.compile(optimizer = Adam(lr = .01, clipnorm = 1.), loss = identity_loss)\n",
    "    siamese_facenet.summary()\n",
    "    return new_model, siamese_facenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_TARGET_SHAPE = (IMAGE_INPUT_SIZE, IMAGE_INPUT_SIZE)\n",
    "    \n",
    "def get_image_tensor(image_path :str, image_target_shape = IMAGE_TARGET_SHAPE):\n",
    "    image_tensor = image.load_img(image_path, target_size = image_target_shape)\n",
    "    image_tensor = image.img_to_array(image_tensor)\n",
    "    image_tensor = np.expand_dims(image_tensor, axis=0)\n",
    "    image_tensor = preprocess_input(image_tensor)\n",
    "    return image_tensor\n",
    "\n",
    "def create_triplets(images_path :str, num_triplets_required = 50000):\n",
    "    images_subdir = []\n",
    "    for subdir in os.listdir(images_path):\n",
    "        if len(images_subdir) == num_triplets_required:\n",
    "            break\n",
    "        filenames = os.listdir(images_path + subdir)\n",
    "        \n",
    "        # Skip directory with single image.\n",
    "        if len(filenames) < 2:\n",
    "            continue\n",
    "        images_subdir.append(subdir)\n",
    "\n",
    "    print(\"Total pairs found : \" + str(len(images_subdir)))\n",
    "    image_input_shape = (len(images_subdir), IMAGE_INPUT_SIZE, IMAGE_INPUT_SIZE, 3)\n",
    "    \n",
    "    anchor_imgs = np.empty(image_input_shape)\n",
    "    pos_imgs = np.empty(image_input_shape)\n",
    "    neg_imgs = np.empty(image_input_shape)\n",
    "    for idx, subdir in enumerate(images_subdir):\n",
    "        filenames = os.listdir(images_path + subdir)\n",
    "        assert len(filenames) > 1\n",
    "        image_path = images_path + subdir + \"/\" + filenames[0]\n",
    "        anchor_imgs[idx] = get_image_tensor(image_path)\n",
    "\n",
    "        image_path = images_path + subdir + \"/\" + filenames[1]\n",
    "        pos_imgs[idx] = get_image_tensor(image_path)\n",
    "\n",
    "    # Rotate pos_imgs by 1 to make them negative for other faces.\n",
    "    neg_imgs = np.roll(pos_imgs, 1, axis = 0)\n",
    "    return (anchor_imgs, pos_imgs, neg_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unmasked_and_masked_triplets(unmasked_images_path :str, masked_images_path :str, \n",
    "                                        num_triplets_required = 50000):\n",
    "\n",
    "    masked_dict = {}\n",
    "    unmasked_path = []\n",
    "    \n",
    "    # Create pairs (subdir, filename) for unmasked images.\n",
    "    for subdir in os.listdir(unmasked_images_path):\n",
    "        filenames = os.listdir(unmasked_images_path + subdir)\n",
    "        filename = unmasked_images_path + subdir + \"/\" + filenames[0]\n",
    "        unmasked_path.append((subdir, filename))\n",
    "\n",
    "    # Create dictionary {subdir : filename} for maksed images to join with unmasked.\n",
    "    for subdir in os.listdir(masked_images_path):\n",
    "        filenames = os.listdir(masked_images_path + subdir)\n",
    "        filename = masked_images_path + subdir + \"/\" + filenames[0]\n",
    "        masked_dict[subdir] = filename\n",
    "\n",
    "    image_pairs = []\n",
    "    for i in range(len(unmasked_path)):\n",
    "        if len(image_pairs) == num_triplets_required:\n",
    "            break\n",
    "        anchor_name, anchor_path = unmasked_path[i]\n",
    "        if anchor_name not in masked_dict:\n",
    "            continue\n",
    "        pos_path = masked_dict[anchor_name]\n",
    "        image_pairs.append((anchor_path, pos_path))\n",
    "        \n",
    "    print(\"Total pairs found : \" + str(len(image_pairs)))\n",
    "    image_input_shape = (len(image_pairs), IMAGE_INPUT_SIZE, IMAGE_INPUT_SIZE, 3)\n",
    "    \n",
    "    anchor_imgs = np.empty(image_input_shape)\n",
    "    pos_imgs = np.empty(image_input_shape)\n",
    "    neg_imgs = np.empty(image_input_shape)\n",
    "    for idx, image_pair in enumerate(image_pairs):\n",
    "        anchor_imgs[idx] = get_image_tensor(image_pair[0])\n",
    "        pos_imgs[idx] = get_image_tensor(image_pair[1])\n",
    "\n",
    "    # Rotate pos_imgs by 1 to make them negative for other faces.\n",
    "    neg_imgs = np.roll(pos_imgs, 1, axis = 0)\n",
    "    return (anchor_imgs, pos_imgs, neg_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs found : 1389\n",
      "Total pairs found : 600\n",
      "Total pairs found : 5721\n",
      "(7710, 160, 160, 3)\n",
      "(7710, 160, 160, 3)\n",
      "(7710, 160, 160, 3)\n",
      "[-103.93900299 -116.77899933 -123.68000031]\n",
      "[-103.93900299 -116.77899933 -123.68000031]\n"
     ]
    }
   ],
   "source": [
    "anchor1, pos1, neg1 = create_triplets(PATH_DATASET_UNMASKED_TRAIN)\n",
    "anchor2, pos2, neg2 = create_triplets(PATH_DATASET_MASKED_TRAIN)\n",
    "anchor3, pos3, neg3 = create_unmasked_and_masked_triplets(PATH_DATASET_UNMASKED_TRAIN, PATH_DATASET_MASKED_TRAIN)\n",
    "\n",
    "anchor_images = np.concatenate((anchor1, anchor2, anchor3), axis = 0)\n",
    "pos_images = np.concatenate((pos1, pos2, pos3), axis = 0)\n",
    "neg_images = np.concatenate((neg1, neg2, neg3), axis = 0)\n",
    "\n",
    "print(anchor_images.shape)\n",
    "print(pos_images.shape)\n",
    "print(neg_images.shape)\n",
    "\n",
    "# Ensure neg_images are rotated version of pos_images\n",
    "print(pos_images[0][0][0])\n",
    "print(neg_images[1][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7710\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:245: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:184: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "426\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "anchor_input (InputLayer)       (None, 160, 160, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pos_input (InputLayer)          (None, 160, 160, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "neg_input (InputLayer)          (None, 160, 160, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 128)          22808400    anchor_input[0][0]               \n",
      "                                                                 pos_input[0][0]                  \n",
      "                                                                 neg_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               ()                   0           model_1[1][0]                    \n",
      "                                                                 model_1[2][0]                    \n",
      "                                                                 model_1[3][0]                    \n",
      "==================================================================================================\n",
      "Total params: 22,808,400\n",
      "Trainable params: 1,039,744\n",
      "Non-trainable params: 21,768,656\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "training_loss = []\n",
    "z = np.zeros(len(anchor_images))\n",
    "print(len(z))\n",
    "\n",
    "new_model, siamese_model = get_siamese_from_facenet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/cpu/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Epoch 1/1\n",
      "7710/7710 [==============================] - 285s 37ms/step - loss: 1.6265\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "facenet_encoding_input (Inpu (None, 160, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 128)               22808400  \n",
      "=================================================================\n",
      "Total params: 22,808,400\n",
      "Trainable params: 1,039,744\n",
      "Non-trainable params: 21,768,656\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "7710/7710 [==============================] - 223s 29ms/step - loss: 1.4086\n",
      "Epoch 1/1\n",
      "7710/7710 [==============================] - 227s 29ms/step - loss: 1.3584\n",
      "Epoch 1/1\n",
      "7710/7710 [==============================] - 224s 29ms/step - loss: 1.3061\n",
      "Epoch 1/1\n",
      "7710/7710 [==============================] - 225s 29ms/step - loss: 1.2703\n",
      "Epoch 1/1\n",
      "7710/7710 [==============================] - 224s 29ms/step - loss: 1.2856\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "facenet_encoding_input (Inpu (None, 160, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 128)               22808400  \n",
      "=================================================================\n",
      "Total params: 22,808,400\n",
      "Trainable params: 1,039,744\n",
      "Non-trainable params: 21,768,656\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "7710/7710 [==============================] - 228s 30ms/step - loss: 1.2261\n",
      "Epoch 1/1\n",
      "7710/7710 [==============================] - 226s 29ms/step - loss: 1.2208\n",
      "Epoch 1/1\n",
      "7710/7710 [==============================] - 226s 29ms/step - loss: 1.2028\n",
      "Epoch 1/1\n",
      "7710/7710 [==============================] - 228s 30ms/step - loss: 1.2125\n",
      "Epoch 1/1\n",
      "7710/7710 [==============================] - 226s 29ms/step - loss: 1.1899\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "facenet_encoding_input (Inpu (None, 160, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 128)               22808400  \n",
      "=================================================================\n",
      "Total params: 22,808,400\n",
      "Trainable params: 1,039,744\n",
      "Non-trainable params: 21,768,656\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "7710/7710 [==============================] - 233s 30ms/step - loss: 1.1841\n",
      "Epoch 1/1\n",
      "7710/7710 [==============================] - 227s 29ms/step - loss: 1.1846\n",
      "Epoch 1/1\n",
      "4288/7710 [===============>..............] - ETA: 1:39 - loss: 1.1729"
     ]
    }
   ],
   "source": [
    "EPOCHS_COUNT = 75\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "for epoch in range(EPOCHS_COUNT):\n",
    "    siamese_model.fit(x=[anchor_images, pos_images, neg_images], \n",
    "                    y=z, \n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    epochs=1, \n",
    "                    verbose=1, \n",
    "                    callbacks=None, \n",
    "                    validation_split=0.0, \n",
    "                    validation_data=None, \n",
    "                    shuffle=True, \n",
    "                    class_weight=None, \n",
    "                    sample_weight=None, \n",
    "                    initial_epoch=0, \n",
    "                    steps_per_epoch=None, \n",
    "                    validation_steps=None)\n",
    "    training_loss.append(siamese_model.history.history['loss'])\n",
    "    \n",
    "    # Rotate negatives in triplets for next epoch.\n",
    "    neg1 = np.roll(neg1, 1, axis = 0)\n",
    "    neg2 = np.roll(neg2, 1, axis = 0)\n",
    "    neg3 = np.roll(neg3, 1, axis = 0)\n",
    "    neg_images = np.concatenate((neg1, neg2, neg3), axis = 0)\n",
    "    \n",
    "    if (epoch % 5 == 0 and training_loss[-1][0] > 0):\n",
    "        # Create and save the Encoding Network to use in predictions.\n",
    "        encoding_input = Input(shape=(IMAGE_INPUT_SIZE, IMAGE_INPUT_SIZE, 3), name='facenet_encoding_input')\n",
    "        encoding_output = new_model(encoding_input)\n",
    "        encoding_facenet = Model(inputs  = encoding_input, outputs = encoding_output)\n",
    "        weights = siamese_model.get_layer('model_1').get_weights()\n",
    "        encoding_facenet.get_layer('model_1').set_weights(weights)\n",
    "        encoding_facenet.summary()\n",
    "        \n",
    "        # Save the Encoding Network architecture\n",
    "        encoding_model_json = encoding_facenet.to_json()\n",
    "        with open(PATH_TRAIN_FACENET + \"/encoding_facenet_arch.json\", \"w\") as json_file:\n",
    "            json_file.write(encoding_model_json)\n",
    "        # Save the Encoding Network model weights    \n",
    "        encoding_facenet.save_weights(PATH_TRAIN_FACENET + '/encoding_facenet_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff077b15128>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "title = 'Triplet Loss'\n",
    "plt.title(title)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.plot(training_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training completed so save the model architecture and weights.\n",
    "# We can save the Siamese FaceNet Network architecture.\n",
    "# siamese_model_json = siamese_facenet.to_json()\n",
    "# with open(PATH_TRAIN_FACENET + \"/siamese_facenet_arch.json\", \"w\") as json_file:\n",
    "#     json_file.write(siamese_model_json)\n",
    "# # save the Siamese Network model weights\n",
    "# siamese_facenet.save_weights(PATH_TRAIN_FACENET + \"/siamese_facenet_weights.h5\")\n",
    "\n",
    "# Create and save the Encoding Network to use in predictions.\n",
    "# encoding_input = Input(shape=(IMAGE_INPUT_SIZE, IMAGE_INPUT_SIZE, 3), name='facenet_encoding_input')\n",
    "# encoding_output = new_model(encoding_input)\n",
    "# encoding_facenet = Model(inputs  = encoding_input, outputs = encoding_output)\n",
    "# weights = siamese_model.get_layer('model_1').get_weights()\n",
    "# encoding_facenet.get_layer('model_1').set_weights(weights)\n",
    "# encoding_facenet.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the Encoding Network architecture\n",
    "# encoding_model_json = encoding_facenet.to_json()\n",
    "# with open(PATH_TRAIN_FACENET + \"/encoding_facenet_arch.json\", \"w\") as json_file:\n",
    "#     json_file.write(encoding_model_json)\n",
    "# # Save the Encoding Network model weights    \n",
    "# encoding_facenet.save_weights(PATH_TRAIN_FACENET + '/encoding_facenet_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"[INFO] Loading Facenet model...\")\n",
    "# with open(PATH_TRAIN_FACENET + '/encoding_facenet_arch.json','r') as f:\n",
    "#     model_json = f.read()\n",
    "\n",
    "# facenet_model = model_from_json(model_json)\n",
    "# facenet_model.load_weights(PATH_TRAIN_FACENET + '/encoding_facenet_weights.h5')\n",
    "# #facenet_model = load_model(PATH_FACENET_KERAS_H5)\n",
    "# print('[INFO] Loaded Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# positive_encodings = []\n",
    "# for i in range(50):\n",
    "#     encoding = encoding_facenet.predict([pos_images[i:i+1]], batch_size = 1, verbose = 0)\n",
    "#     positive_encodings.append(encoding)\n",
    "\n",
    "# for i in range(50):\n",
    "#     anchor_encoding = encoding_facenet.predict([anchor_images[i:i+1]], batch_size = 1, verbose = 0)\n",
    "#     min_distance = 10000\n",
    "#     min_index = -1\n",
    "#     for j in range(len(positive_encodings)):\n",
    "#         if i == j:\n",
    "#             continue\n",
    "#         distance = np.linalg.norm(anchor_encoding - positive_encodings[j])\n",
    "#         if distance < min_distance:\n",
    "#             min_distance = distance\n",
    "#             min_index = j\n",
    "    \n",
    "#     pos_distance = np.linalg.norm(anchor_encoding - positive_encodings[i])\n",
    "#     if (pos_distance <= min_distance):\n",
    "#         print(str(i) + \" postive distance   : \" + str(pos_distance))\n",
    "#         print(str(min_index) + \" negative distance : \" + str(min_distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
