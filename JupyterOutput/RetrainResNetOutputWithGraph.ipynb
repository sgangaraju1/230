{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/cpu/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import scipy\n",
    "import shutil\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.layers import Dense, Lambda, Input, BatchNormalization\n",
    "from keras.models import load_model, Model, model_from_json\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.preprocessing import image\n",
    "from keras import backend as K\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "INPUTS_FNAME   = \"lfw_datasets_and_models.zip\"\n",
    "\n",
    "PATH_INPUTS_FNAME     = \"./lfw_datasets_and_models.zip\" \n",
    "PATH_INPUTS           = \"./lfw_datasets_and_models\"\n",
    "\n",
    "PATH_DATASET_BASE     = PATH_INPUTS + \"/datasets\"\n",
    "\n",
    "PATH_DATASET_BASE_MASKED   = PATH_DATASET_BASE + \"/masked\"\n",
    "PATH_DATASET_BASE_UNMASKED = PATH_DATASET_BASE + \"/unmasked\"\n",
    "\n",
    "PATH_DATASET_MASKED_TRAIN = PATH_DATASET_BASE_MASKED + \"/train/\"\n",
    "PATH_DATASET_MASKED_VAL   = PATH_DATASET_BASE_MASKED + \"/validation/\"\n",
    "PATH_DATASET_MASKED_TEST  = PATH_DATASET_BASE_MASKED + \"/test/\"\n",
    "\n",
    "PATH_DATASET_UNMASKED_TRAIN = PATH_DATASET_BASE_UNMASKED + \"/train/\"\n",
    "PATH_DATASET_UNMASKED_VAL   = PATH_DATASET_BASE_UNMASKED + \"/validation/\"\n",
    "PATH_DATASET_UNMASKED_TEST  = PATH_DATASET_BASE_UNMASKED + \"/test/\"\n",
    "\n",
    "PATH_TRAIN_RESNET = PATH_INPUTS + '/models/resnet/retrained/'\n",
    "PATH_TRAIN_FACENET = PATH_INPUTS + '/models/facenet/retrained/'\n",
    "PATH_TRAIN_DATASET = PATH_INPUTS + '/training'\n",
    "PATH_TRAIN_ANCHOR_DATASET = PATH_TRAIN_DATASET + '/anchor/'\n",
    "PATH_TRAIN_POSITIVE_DATASET = PATH_TRAIN_DATASET + '/positive/'\n",
    "\n",
    "PATH_FACENET_KERAS_H5 = PATH_INPUTS + \"/models/facenet/pretrained/model/facenet_keras.h5\"\n",
    "\n",
    "TRIPLET_LOSS_MARGIN = 2\n",
    "\n",
    "IMAGE_INPUT_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_zipfile(filename: str, extract_dirname: str, extract_path: str):\n",
    "    # Extract the inputs from the zip file.\n",
    "    if (not os.path.isdir(extract_dirname)):\n",
    "        print(\"[INFO] Extracting from '{}' to '../'...\".format(filename), end=\" \")\n",
    "        with zipfile.ZipFile(filename, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "        print(\"DONE.\")\n",
    "    else:\n",
    "        print(\"[INFO] Directory '{}' exists.\".format(extract_dirname))\n",
    "\n",
    "# extract_zipfile(PATH_INPUTS_FNAME, PATH_INPUTS, \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [Retraing]\n",
    "def triplet_loss(inputs, dist='euclidean', margin='maxplus'):\n",
    "    anchor, positive, negative = inputs\n",
    "    positive_distance = K.square(anchor - positive)\n",
    "    negative_distance = K.square(anchor - negative)\n",
    "    if dist == 'euclidean':\n",
    "        positive_distance = K.sqrt(K.sum(positive_distance, axis=-1, keepdims=True))\n",
    "        negative_distance = K.sqrt(K.sum(negative_distance, axis=-1, keepdims=True))\n",
    "    elif dist == 'sqeuclidean':\n",
    "        positive_distance = K.sum(positive_distance, axis=-1, keepdims=True)\n",
    "        negative_distance = K.sum(negative_distance, axis=-1, keepdims=True)\n",
    "    loss = positive_distance - negative_distance\n",
    "    if margin == 'maxplus':\n",
    "        loss = K.maximum(0.0, TRIPLET_LOSS_MARGIN + loss)\n",
    "    elif margin == 'softplus':\n",
    "        loss = K.log(1 + K.exp(loss))\n",
    "        \n",
    "    returned_loss = K.mean(loss)\n",
    "    return returned_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [Retraining]\n",
    "# Used when compiling the siamese network\n",
    "def identity_loss(y_true, y_pred):\n",
    "    return K.mean(y_pred - 0 * y_true)  # This is actually just returning y_pred bcs\n",
    "                                        # K.mean has already been called in the triplet_loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# [Retraining]\n",
    "def get_siamese_from_resnet50():\n",
    "    model = ResNet50(weights='imagenet')\n",
    "    print(len(model.layers))\n",
    "    model.layers.pop()\n",
    "    #model.summary()\n",
    "\n",
    "    layers_count = len(model.layers)\n",
    "    for i in range(layers_count - 1):\n",
    "        layer = model.layers[i]\n",
    "        if i < layers_count//2:\n",
    "            layer.trainable = False # Freeze first half of layers.\n",
    "        elif layer.trainable and not layer.name.startswith(\"bn\"):\n",
    "            layer.trainable = False # Leave all BatchNorm layers to retrain.\n",
    "\n",
    "    last_layer = model.layers[layers_count - 1]\n",
    "    assert last_layer.trainable == True # Ensure last layer to retrain.\n",
    "\n",
    "    # Define the siamese facenet network\n",
    "    image_shape = (IMAGE_INPUT_SIZE, IMAGE_INPUT_SIZE, 3)\n",
    "\n",
    "    model_out = last_layer.output\n",
    "    model_out = Dense(128, activation = 'relu',  name = 'model_out')(model_out)\n",
    "    model_out = BatchNormalization(axis = 1, epsilon=0.00001, name = 'BatchNorm_last')(model_out)\n",
    "    model_out = Lambda(lambda  x: K.l2_normalize(x, axis = 1))(model_out)\n",
    "\n",
    "    new_model = Model(inputs=model.input, outputs=model_out)\n",
    "\n",
    "    anchor_input = Input(shape=image_shape, name='anchor_input')\n",
    "    pos_input = Input(shape=image_shape, name='pos_input')\n",
    "    neg_input = Input(shape=image_shape, name='neg_input')\n",
    "\n",
    "    encoding_anchor = new_model(anchor_input)\n",
    "    encoding_pos = new_model(pos_input)\n",
    "    encoding_neg = new_model(neg_input)\n",
    "\n",
    "    loss = Lambda(triplet_loss)([encoding_anchor, encoding_pos, encoding_neg])\n",
    "\n",
    "    siamese_facenet = Model(inputs  = [anchor_input, pos_input, neg_input], outputs = loss)\n",
    "    siamese_facenet.compile(optimizer=Adam(lr = .01, clipnorm = 1.), loss = identity_loss)\n",
    "    siamese_facenet.summary()\n",
    "    return new_model, siamese_facenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_TARGET_SHAPE = (IMAGE_INPUT_SIZE, IMAGE_INPUT_SIZE)\n",
    "    \n",
    "def get_image_tensor(image_path :str, image_target_shape = IMAGE_TARGET_SHAPE):\n",
    "    image_tensor = image.load_img(image_path, target_size = image_target_shape)\n",
    "    image_tensor = image.img_to_array(image_tensor)\n",
    "    image_tensor = np.expand_dims(image_tensor, axis=0)\n",
    "    image_tensor = preprocess_input(image_tensor)\n",
    "    return image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_triplets(images_path :str, num_triplets_required = 50000):\n",
    "    images_subdir = []\n",
    "    for subdir in os.listdir(images_path):\n",
    "        if len(images_subdir) == num_triplets_required:\n",
    "            break\n",
    "        filenames = os.listdir(images_path + subdir)\n",
    "        \n",
    "        # Skip directory with single image.\n",
    "        if len(filenames) < 2:\n",
    "            continue\n",
    "        images_subdir.append(subdir)\n",
    "\n",
    "    print(\"Total pairs found : \" + str(len(images_subdir)))\n",
    "    image_input_shape = (len(images_subdir), IMAGE_INPUT_SIZE, IMAGE_INPUT_SIZE, 3)\n",
    "    \n",
    "    anchor_imgs = np.empty(image_input_shape)\n",
    "    pos_imgs = np.empty(image_input_shape)\n",
    "    neg_imgs = np.empty(image_input_shape)\n",
    "    for idx, subdir in enumerate(images_subdir):\n",
    "        filenames = os.listdir(images_path + subdir)\n",
    "        assert len(filenames) > 1\n",
    "        image_path = images_path + subdir + \"/\" + filenames[0]\n",
    "        anchor_imgs[idx] = get_image_tensor(image_path)\n",
    "\n",
    "        image_path = images_path + subdir + \"/\" + filenames[1]\n",
    "        pos_imgs[idx] = get_image_tensor(image_path)\n",
    "\n",
    "    # Rotate pos_imgs by 1 to make them negative for other faces.\n",
    "    neg_imgs = np.roll(pos_imgs, 1, axis = 0)\n",
    "    return (anchor_imgs, pos_imgs, neg_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unmasked_and_masked_triplets(unmasked_images_path :str, masked_images_path :str, \n",
    "                                        num_triplets_required = 50000):\n",
    "    masked_dict = {}\n",
    "    unmasked_path = []\n",
    "    \n",
    "    # Create pairs (subdir, filename) for unmasked images.\n",
    "    for subdir in os.listdir(unmasked_images_path):\n",
    "        filenames = os.listdir(unmasked_images_path + subdir)\n",
    "        filename = unmasked_images_path + subdir + \"/\" + filenames[0]\n",
    "        unmasked_path.append((subdir, filename))\n",
    "\n",
    "    # Create dictionary {subdir : filename} for maksed images to join with unmasked.\n",
    "    for subdir in os.listdir(masked_images_path):\n",
    "        filenames = os.listdir(masked_images_path + subdir)\n",
    "        filename = masked_images_path + subdir + \"/\" + filenames[0]\n",
    "        masked_dict[subdir] = filename\n",
    "\n",
    "    image_pairs = []\n",
    "    for i in range(len(unmasked_path)):\n",
    "        if len(image_pairs) == num_triplets_required:\n",
    "            break\n",
    "        anchor_name, anchor_path = unmasked_path[i]\n",
    "        if anchor_name not in masked_dict:\n",
    "            continue\n",
    "        pos_path = masked_dict[anchor_name]\n",
    "        image_pairs.append((anchor_path, pos_path))\n",
    "        \n",
    "    print(\"Total pairs found : \" + str(len(image_pairs)))\n",
    "    image_input_shape = (len(image_pairs), IMAGE_INPUT_SIZE, IMAGE_INPUT_SIZE, 3)\n",
    "    \n",
    "    anchor_imgs = np.empty(image_input_shape)\n",
    "    pos_imgs = np.empty(image_input_shape)\n",
    "    neg_imgs = np.empty(image_input_shape)\n",
    "    for idx, image_pair in enumerate(image_pairs):\n",
    "        anchor_imgs[idx] = get_image_tensor(image_pair[0])\n",
    "        pos_imgs[idx] = get_image_tensor(image_pair[1])\n",
    "\n",
    "    # Rotate pos_imgs by 1 to make them negative for other faces.\n",
    "    neg_imgs = np.roll(pos_imgs, 1, axis = 0)\n",
    "    return (anchor_imgs, pos_imgs, neg_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs found : 1389\n",
      "Total pairs found : 600\n",
      "Total pairs found : 5721\n",
      "(7710, 224, 224, 3)\n",
      "(7710, 224, 224, 3)\n",
      "(7710, 224, 224, 3)\n",
      "[-103.93900299 -115.77899933 -123.68000031]\n",
      "[-103.93900299 -115.77899933 -123.68000031]\n"
     ]
    }
   ],
   "source": [
    "anchor1, pos1, neg1 = create_triplets(PATH_DATASET_UNMASKED_TRAIN)\n",
    "anchor2, pos2, neg2 = create_triplets(PATH_DATASET_MASKED_TRAIN)\n",
    "anchor3, pos3, neg3 = create_unmasked_and_masked_triplets(PATH_DATASET_UNMASKED_TRAIN, PATH_DATASET_MASKED_TRAIN)\n",
    "\n",
    "anchor_images = np.concatenate((anchor1, anchor2, anchor3), axis = 0)\n",
    "pos_images = np.concatenate((pos1, pos2, pos3), axis = 0)\n",
    "neg_images = np.concatenate((neg1, neg2, neg3), axis = 0)\n",
    "\n",
    "print(anchor_images.shape)\n",
    "print(pos_images.shape)\n",
    "print(neg_images.shape)\n",
    "\n",
    "# Ensure neg_images are rotated version of pos_images\n",
    "print(pos_images[0][0][0])\n",
    "print(neg_images[1][0][0])\n",
    "\n",
    "training_loss = []\n",
    "z = np.zeros(len(anchor_images))\n",
    "# print(len(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:184: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "177\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "anchor_input (InputLayer)       (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pos_input (InputLayer)          (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "neg_input (InputLayer)          (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 128)          23850496    anchor_input[0][0]               \n",
      "                                                                 pos_input[0][0]                  \n",
      "                                                                 neg_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               ()                   0           model_1[1][0]                    \n",
      "                                                                 model_1[2][0]                    \n",
      "                                                                 model_1[3][0]                    \n",
      "==================================================================================================\n",
      "Total params: 23,850,496\n",
      "Trainable params: 304,512\n",
      "Non-trainable params: 23,545,984\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "new_model, siamese_model = get_siamese_from_resnet50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "7710/7710 [==============================] - 441s 57ms/step - loss: 1.0681\n",
      "Epoch 1/1\n",
      "3520/7710 [============>.................] - ETA: 4:00 - loss: 1.0602"
     ]
    }
   ],
   "source": [
    "EPOCHS_COUNT = 75\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "for epoch in range(EPOCHS_COUNT):\n",
    "    siamese_model.fit(x=[anchor_images, pos_images, neg_images], \n",
    "                    y=z, \n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    epochs=1, \n",
    "                    verbose=1, \n",
    "                    callbacks=None, \n",
    "                    validation_split=0.0, \n",
    "                    validation_data=None, \n",
    "                    shuffle=True, \n",
    "                    class_weight=None, \n",
    "                    sample_weight=None, \n",
    "                    initial_epoch=0, \n",
    "                    steps_per_epoch=None, \n",
    "                    validation_steps=None)\n",
    "    training_loss.append(siamese_model.history.history['loss'])\n",
    "    \n",
    "    # Rotate negatives in triplets for next epoch.\n",
    "    neg1 = np.roll(neg1, 1, axis = 0)\n",
    "    neg2 = np.roll(neg2, 1, axis = 0)\n",
    "    neg3 = np.roll(neg3, 1, axis = 0)\n",
    "    neg_images = np.concatenate((neg1, neg2, neg3), axis = 0)\n",
    "    \n",
    "    if (epoch % 5 == 0 and training_loss[-1][0] > 0):\n",
    "        # Create and save the Encoding Network to use in predictions.\n",
    "        encoding_input = Input(shape=(IMAGE_INPUT_SIZE, IMAGE_INPUT_SIZE, 3), name='resnet_encoding_input')\n",
    "        encoding_output = new_model(encoding_input)\n",
    "        encoding_resnet = Model(inputs  = encoding_input, outputs = encoding_output)\n",
    "\n",
    "        weights = siamese_model.get_layer('model_1').get_weights()\n",
    "        encoding_resnet.get_layer('model_1').set_weights(weights)\n",
    "\n",
    "        # Save the Encoding Network architecture\n",
    "        encoding_model_json = encoding_resnet.to_json()\n",
    "        with open(PATH_TRAIN_RESNET + \"/encoding_resnet_arch.json\", \"w\") as json_file:\n",
    "            json_file.write(encoding_model_json)\n",
    "        # save the Encoding Network model weights    \n",
    "        encoding_resnet.save_weights(PATH_TRAIN_RESNET + '/encoding_resnet_weights.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb4f62d3588>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "title = 'Triplet Loss'\n",
    "plt.title(title)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.plot(training_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Training completed at this point. Save the model architecture and weights.\n",
    "# # Save the Siamese ResNet50 Network architecture\n",
    "# siamese_model_json = siamese_model.to_json()\n",
    "# with open(PATH_TRAIN_RESNET + \"/siamese_resnet_arch.json\", \"w\") as json_file:\n",
    "#     json_file.write(siamese_model_json)\n",
    "# # save the Siamese Network model weights\n",
    "# siamese_model.save_weights(PATH_TRAIN_RESNET + \"/siamese_resnet_weights.h5\")\n",
    "\n",
    "# # create and save the Encoding Network to use in predictions later on\n",
    "# encoding_input = Input(shape=(IMAGE_INPUT_SIZE, IMAGE_INPUT_SIZE, 3), name='resnet_encoding_input')\n",
    "# encoding_output = new_model(encoding_input)\n",
    "# encoding_resnet = Model(inputs  = encoding_input, outputs = encoding_output)\n",
    "\n",
    "# weights = siamese_model.get_layer('model_1').get_weights()\n",
    "# encoding_resnet.get_layer('model_1').set_weights(weights)\n",
    "\n",
    "# # Save the Encoding Network architecture\n",
    "# encoding_model_json = encoding_resnet.to_json()\n",
    "# with open(PATH_TRAIN_RESNET + \"/encoding_resnet_arch.json\", \"w\") as json_file:\n",
    "#     json_file.write(encoding_model_json)\n",
    "# # save the Encoding Network model weights    \n",
    "# encoding_resnet.save_weights(PATH_TRAIN_RESNET + '/encoding_resnet_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# positive_encodings = []\n",
    "# for i in range(len(pos_images)):\n",
    "#     encoding = encoding_facenet.predict([pos_images[i:i+1]], batch_size = 1, verbose = 0)\n",
    "#     positive_encodings.append(encoding)\n",
    "\n",
    "# for i in range(25):\n",
    "#     anchor_encoding = encoding_facenet.predict([anchor_images[i:i+1]], batch_size = 1, verbose = 0)\n",
    "#     min_distance = 10000\n",
    "#     min_index = -1\n",
    "#     for j in range(len(positive_encodings)):\n",
    "#         if i == j:\n",
    "#             continue\n",
    "#         distance = np.linalg.norm(anchor_encoding - positive_encodings[j])\n",
    "#         assert distance >= 0\n",
    "#         if distance < min_distance:\n",
    "#             min_distance = distance\n",
    "#             min_index = j\n",
    "    \n",
    "#     pos_distance = np.linalg.norm(anchor_encoding - positive_encodings[i])\n",
    "#     if (pos_distance <= min_distance):\n",
    "#         print(str(i) + \" postive distance   : \" + str(pos_distance))\n",
    "#         print(str(min_index) + \" negative distance : \" + str(min_distance))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
