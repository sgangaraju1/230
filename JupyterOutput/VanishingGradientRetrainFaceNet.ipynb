{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/cpu/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import scipy\n",
    "import shutil\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.layers import Dense, Lambda, Input\n",
    "from keras.models import load_model, Model, model_from_json\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.preprocessing import image\n",
    "from keras import backend as K\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "INPUTS_FNAME   = \"lfw_datasets_and_models.zip\"\n",
    "\n",
    "PATH_INPUTS_FNAME     = \"./lfw_datasets_and_models.zip\" \n",
    "PATH_INPUTS           = \"./test/lfw_datasets_and_models\"\n",
    "\n",
    "PATH_DATASET_BASE     = PATH_INPUTS + \"/datasets\"\n",
    "\n",
    "PATH_DATASET_BASE_MASKED   = PATH_DATASET_BASE + \"/masked\"\n",
    "PATH_DATASET_BASE_UNMASKED = PATH_DATASET_BASE + \"/unmasked\"\n",
    "\n",
    "PATH_DATASET_MASKED_TRAIN = PATH_DATASET_BASE_MASKED + \"/train/\"\n",
    "PATH_DATASET_MASKED_VAL   = PATH_DATASET_BASE_MASKED + \"/validation/\"\n",
    "PATH_DATASET_MASKED_TEST  = PATH_DATASET_BASE_MASKED + \"/test/\"\n",
    "\n",
    "PATH_DATASET_UNMASKED_TRAIN = PATH_DATASET_BASE_UNMASKED + \"/train/\"\n",
    "PATH_DATASET_UNMASKED_VAL   = PATH_DATASET_BASE_UNMASKED + \"/validation/\"\n",
    "PATH_DATASET_UNMASKED_TEST  = PATH_DATASET_BASE_UNMASKED + \"/test/\"\n",
    "\n",
    "PATH_TRAIN_RESNET = PATH_INPUTS + '/models/resnet'\n",
    "PATH_TRAIN_FACENET = PATH_INPUTS + '/models/facenet'\n",
    "PATH_TRAIN_DATASET = PATH_INPUTS + '/training'\n",
    "PATH_TRAIN_ANCHOR_DATASET = PATH_TRAIN_DATASET + '/anchor/'\n",
    "PATH_TRAIN_POSITIVE_DATASET = PATH_TRAIN_DATASET + '/positive/'\n",
    "\n",
    "PATH_FACENET_KERAS_H5 = PATH_INPUTS + \"/models/facenet/pretrained/model/facenet_keras.h5\"\n",
    "\n",
    "TRIPLET_LOSS_MARGIN = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_zipfile(filename: str, extract_dirname: str, extract_path: str):\n",
    "    # Extract the inputs from the zip file.\n",
    "    if (not os.path.isdir(extract_dirname)):\n",
    "        print(\"[INFO] Extracting from '{}' to '../'...\".format(filename), end=\" \")\n",
    "        with zipfile.ZipFile(filename, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "        print(\"DONE.\")\n",
    "    else:\n",
    "        print(\"[INFO] Directory '{}' exists.\".format(extract_dirname))\n",
    "\n",
    "#extract_zipfile(PATH_INPUTS_FNAME, PATH_INPUTS, \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [Retraing]\n",
    "def triplet_loss(inputs, dist='euclidean', margin='maxplus'):\n",
    "    anchor, positive, negative = inputs\n",
    "    positive_distance = K.square(anchor - positive)\n",
    "    negative_distance = K.square(anchor - negative)\n",
    "    if dist == 'euclidean':\n",
    "        positive_distance = K.sqrt(K.sum(positive_distance, axis=-1, keepdims=True))\n",
    "        negative_distance = K.sqrt(K.sum(negative_distance, axis=-1, keepdims=True))\n",
    "    elif dist == 'sqeuclidean':\n",
    "        positive_distance = K.sum(positive_distance, axis=-1, keepdims=True)\n",
    "        negative_distance = K.sum(negative_distance, axis=-1, keepdims=True)\n",
    "    loss = positive_distance - negative_distance\n",
    "    if margin == 'maxplus':\n",
    "        loss = K.maximum(0.0, TRIPLET_LOSS_MARGIN + loss)\n",
    "    elif margin == 'softplus':\n",
    "        loss = K.log(1 + K.exp(loss))\n",
    "        \n",
    "    returned_loss = K.mean(loss)\n",
    "    return returned_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [Retraining]\n",
    "# Used when compiling the siamese network\n",
    "def identity_loss(y_true, y_pred):\n",
    "    return K.mean(y_pred - 0 * y_true)  # This is actually just returning y_pred bcs\n",
    "                                        # K.mean has already been called in the triplet_loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# [Retraining]\n",
    "def get_siamese_from_facenet():\n",
    "    model = load_model(PATH_FACENET_KERAS_H5)\n",
    "    print(len(model.layers))\n",
    "    model.layers.pop()\n",
    "    model.layers.pop()\n",
    "    #model.summary()\n",
    "\n",
    "    layers_count = len(model.layers)\n",
    "    for i in range(layers_count - 1):\n",
    "        layer = model.layers[i]\n",
    "        if i < layers_count//2:\n",
    "            layer.trainable = False # Freeze first half of layers.\n",
    "        elif layer.trainable and not layer.name.endswith(\"BatchNorm\"):\n",
    "            layer.trainable = False # Leave all BatchNorm layers to retrain.\n",
    "\n",
    "    last_layer = model.layers[layers_count - 1]\n",
    "    assert last_layer.trainable == True # Ensure last layer to retrain.\n",
    "\n",
    "    # Define the siamese facenet network\n",
    "    image_shape = (160, 160, 3)\n",
    "\n",
    "    x = last_layer.output\n",
    "    model_out = Dense(128, activation='relu',  name='model_out')(x)\n",
    "    model_out = Lambda(lambda  x: K.l2_normalize(x, axis=-1))(model_out)\n",
    "\n",
    "    new_model = Model(inputs=model.input, outputs=model_out)\n",
    "\n",
    "    anchor_input = Input(shape=image_shape, name='anchor_input')\n",
    "    pos_input = Input(shape=image_shape, name='pos_input')\n",
    "    neg_input = Input(shape=image_shape, name='neg_input')\n",
    "\n",
    "    encoding_anchor = new_model(anchor_input)\n",
    "    encoding_pos = new_model(pos_input)\n",
    "    encoding_neg = new_model(neg_input)\n",
    "\n",
    "    loss = Lambda(triplet_loss)([encoding_anchor, encoding_pos, encoding_neg])\n",
    "\n",
    "    siamese_facenet = Model(inputs  = [anchor_input, pos_input, neg_input], outputs = loss)\n",
    "    siamese_facenet.compile(optimizer=Adam(lr=.02, clipnorm=1.), loss=identity_loss)\n",
    "    siamese_facenet.summary()\n",
    "    return new_model, siamese_facenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_TARGET_SHAPE = (160, 160)\n",
    "    \n",
    "def get_image_tensor(image_path :str, image_target_shape = IMAGE_TARGET_SHAPE):\n",
    "    image_tensor = image.load_img(image_path, target_size = image_target_shape)\n",
    "    image_tensor = image.img_to_array(image_tensor)\n",
    "    image_tensor = np.expand_dims(image_tensor, axis=0)\n",
    "    image_tensor = preprocess_input(image_tensor)\n",
    "    return image_tensor\n",
    "\n",
    "def create_triplets(images_path :str, num_triplets_required = 50000):\n",
    "    images_subdir = []\n",
    "    for subdir in os.listdir(images_path):\n",
    "        if len(images_subdir) == num_triplets_required:\n",
    "            break\n",
    "        filenames = os.listdir(images_path + subdir)\n",
    "        \n",
    "        # Skip directory with single image.\n",
    "        if len(filenames) < 2:\n",
    "            continue\n",
    "        images_subdir.append(subdir)\n",
    "\n",
    "    print(\"Total pairs found : \" + str(len(images_subdir)))\n",
    "    image_input_shape = (len(images_subdir), 160, 160, 3)\n",
    "    \n",
    "    anchor_imgs = np.empty(image_input_shape)\n",
    "    pos_imgs = np.empty(image_input_shape)\n",
    "    neg_imgs = np.empty(image_input_shape)\n",
    "    for idx, subdir in enumerate(images_subdir):\n",
    "        filenames = os.listdir(images_path + subdir)\n",
    "        assert len(filenames) > 1\n",
    "        image_path = images_path + subdir + \"/\" + filenames[0]\n",
    "        anchor_imgs[idx] = get_image_tensor(image_path)\n",
    "\n",
    "        image_path = images_path + subdir + \"/\" + filenames[1]\n",
    "        pos_imgs[idx] = get_image_tensor(image_path)\n",
    "\n",
    "    # Rotate pos_imgs by 1 to make them negative for other faces.\n",
    "    neg_imgs = np.roll(pos_imgs, 1, axis = 0)\n",
    "    return (anchor_imgs, pos_imgs, neg_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unmasked_and_masked_triplets(unmasked_images_path :str, masked_images_path :str, \n",
    "                                        num_triplets_required = 50000):\n",
    "\n",
    "    masked_dict = {}\n",
    "    unmasked_path = []\n",
    "    \n",
    "    # Create pairs (subdir, filename) for unmasked images.\n",
    "    for subdir in os.listdir(unmasked_images_path):\n",
    "        filenames = os.listdir(unmasked_images_path + subdir)\n",
    "        filename = unmasked_images_path + subdir + \"/\" + filenames[0]\n",
    "        unmasked_path.append((subdir, filename))\n",
    "\n",
    "    # Create dictionary {subdir : filename} for maksed images to join with unmasked.\n",
    "    for subdir in os.listdir(masked_images_path):\n",
    "        filenames = os.listdir(masked_images_path + subdir)\n",
    "        filename = masked_images_path + subdir + \"/\" + filenames[0]\n",
    "        masked_dict[subdir] = filename\n",
    "\n",
    "    image_pairs = []\n",
    "    for i in range(len(unmasked_path)):\n",
    "        if len(image_pairs) == num_triplets_required:\n",
    "            break\n",
    "        anchor_name, anchor_path = unmasked_path[i]\n",
    "        if anchor_name not in masked_dict:\n",
    "            continue\n",
    "        pos_path = masked_dict[anchor_name]\n",
    "        image_pairs.append((anchor_path, pos_path))\n",
    "        \n",
    "    print(\"Total pairs found : \" + str(len(image_pairs)))\n",
    "    image_input_shape = (len(image_pairs), 160, 160, 3)\n",
    "    \n",
    "    anchor_imgs = np.empty(image_input_shape)\n",
    "    pos_imgs = np.empty(image_input_shape)\n",
    "    neg_imgs = np.empty(image_input_shape)\n",
    "    for idx, image_pair in enumerate(image_pairs):\n",
    "        anchor_imgs[idx] = get_image_tensor(image_pair[0])\n",
    "        pos_imgs[idx] = get_image_tensor(image_pair[1])\n",
    "\n",
    "    # Rotate pos_imgs by 1 to make them negative for other faces.\n",
    "    neg_imgs = np.roll(pos_imgs, 1, axis = 0)\n",
    "    return (anchor_imgs, pos_imgs, neg_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs found : 1389\n",
      "Total pairs found : 600\n",
      "Total pairs found : 5721\n",
      "(7710, 160, 160, 3)\n",
      "(7710, 160, 160, 3)\n",
      "(7710, 160, 160, 3)\n",
      "[-103.93900299 -116.77899933 -123.68000031]\n",
      "[-103.93900299 -116.77899933 -123.68000031]\n"
     ]
    }
   ],
   "source": [
    "anchor1, pos1, neg1 = create_triplets(PATH_DATASET_UNMASKED_TRAIN)\n",
    "anchor2, pos2, neg2 = create_triplets(PATH_DATASET_MASKED_TRAIN)\n",
    "anchor3, pos3, neg3 = create_unmasked_and_masked_triplets(PATH_DATASET_UNMASKED_TRAIN, PATH_DATASET_MASKED_TRAIN)\n",
    "\n",
    "anchor_images = np.concatenate((anchor1, anchor2, anchor3), axis = 0)\n",
    "pos_images = np.concatenate((pos1, pos2, pos3), axis = 0)\n",
    "neg_images = np.concatenate((neg1, neg2, neg3), axis = 0)\n",
    "\n",
    "print(anchor_images.shape)\n",
    "print(pos_images.shape)\n",
    "print(neg_images.shape)\n",
    "\n",
    "# Ensure neg_images are rotated version of pos_images\n",
    "print(pos_images[0][0][0])\n",
    "print(neg_images[1][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:245: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:184: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "426\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "anchor_input (InputLayer)       (None, 160, 160, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pos_input (InputLayer)          (None, 160, 160, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "neg_input (InputLayer)          (None, 160, 160, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 128)          22807888    anchor_input[0][0]               \n",
      "                                                                 pos_input[0][0]                  \n",
      "                                                                 neg_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               ()                   0           model_1[1][0]                    \n",
      "                                                                 model_1[2][0]                    \n",
      "                                                                 model_1[3][0]                    \n",
      "==================================================================================================\n",
      "Total params: 22,807,888\n",
      "Trainable params: 238,976\n",
      "Non-trainable params: 22,568,912\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/cpu/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Epoch 1/1\n",
      "7710/7710 [==============================] - 274s 36ms/step - loss: 1.6927\n",
      "Epoch 1/1\n",
      "7710/7710 [==============================] - 215s 28ms/step - loss: 1.4971\n",
      "Epoch 1/1\n",
      "7710/7710 [==============================] - 215s 28ms/step - loss: 1.4499\n",
      "Epoch 1/1\n",
      "7710/7710 [==============================] - 217s 28ms/step - loss: 1.4247\n",
      "Epoch 1/1\n",
      "7710/7710 [==============================] - 216s 28ms/step - loss: 1.3983\n",
      "Epoch 1/1\n",
      "7710/7710 [==============================] - 216s 28ms/step - loss: 1.3920\n",
      "Epoch 1/1\n",
      "7710/7710 [==============================] - 215s 28ms/step - loss: 1.3838\n",
      "Epoch 1/1\n",
      "7710/7710 [==============================] - 217s 28ms/step - loss: 0.2460\n",
      "Epoch 1/1\n",
      "7710/7710 [==============================] - 217s 28ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "7710/7710 [==============================] - 216s 28ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "7710/7710 [==============================] - 216s 28ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1920/7710 [======>.......................] - ETA: 2:40 - loss: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0abe4c025c3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m                     \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                     validation_steps=None)\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mtraining_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msiamese_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/cpu/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "EPOCHS_COUNT = 12\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "training_loss = []\n",
    "z = np.zeros(len(anchor_images))\n",
    "# print(len(z))\n",
    "\n",
    "new_model, siamese_model = get_siamese_from_facenet()\n",
    "for epoch in range(EPOCHS_COUNT):\n",
    "    siamese_model.fit(x=[anchor_images, pos_images, neg_images], \n",
    "                    y=z, \n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    epochs=1, \n",
    "                    verbose=1, \n",
    "                    callbacks=None, \n",
    "                    validation_split=0.0, \n",
    "                    validation_data=None, \n",
    "                    shuffle=True, \n",
    "                    class_weight=None, \n",
    "                    sample_weight=None, \n",
    "                    initial_epoch=0, \n",
    "                    steps_per_epoch=None, \n",
    "                    validation_steps=None)\n",
    "    training_loss.append(siamese_model.history.history['loss'])\n",
    "    \n",
    "    # Rotate negatives in triplets for next epoch.\n",
    "    neg1 = np.roll(neg1, 1, axis = 0)\n",
    "    neg2 = np.roll(neg2, 1, axis = 0)\n",
    "    neg3 = np.roll(neg3, 1, axis = 0)\n",
    "    neg_images = np.concatenate((neg1, neg2, neg3), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd476ce1588>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(training_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training completed so save the model architecture and weights.\n",
    "# We can save the Siamese FaceNet Network architecture.\n",
    "# siamese_model_json = siamese_facenet.to_json()\n",
    "# with open(PATH_TRAIN_FACENET + \"/siamese_facenet_arch.json\", \"w\") as json_file:\n",
    "#     json_file.write(siamese_model_json)\n",
    "# # save the Siamese Network model weights\n",
    "# siamese_facenet.save_weights(PATH_TRAIN_FACENET + \"/siamese_facenet_weights.h5\")\n",
    "\n",
    "# Create and save the Encoding Network to use in predictions.\n",
    "encoding_input = Input(shape=(160, 160, 3), name='facenet_encoding_input')\n",
    "encoding_output = new_model(encoding_input)\n",
    "encoding_facenet = Model(inputs  = encoding_input, outputs = encoding_output)\n",
    "weights = siamese_model.get_layer('model_1').get_weights()\n",
    "encoding_facenet.get_layer('model_1').set_weights(weights)\n",
    "encoding_facenet.summary()\n",
    "\n",
    "# Save the Encoding Network architecture\n",
    "# encoding_model_json = encoding_facenet.to_json()\n",
    "# with open(PATH_TRAIN_FACENET + \"/encoding_facenet_arch.json\", \"w\") as json_file:\n",
    "#     json_file.write(encoding_model_json)\n",
    "# # Save the Encoding Network model weights    \n",
    "# encoding_facenet.save_weights(PATH_TRAIN_FACENET + '/encoding_facenet_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"[INFO] Loading Facenet model...\")\n",
    "# with open(PATH_TRAIN_FACENET + '/encoding_facenet_arch.json','r') as f:\n",
    "#     model_json = f.read()\n",
    "\n",
    "# facenet_model = model_from_json(model_json)\n",
    "# facenet_model.load_weights(PATH_TRAIN_FACENET + '/encoding_facenet_weights.h5')\n",
    "# #facenet_model = load_model(PATH_FACENET_KERAS_H5)\n",
    "# print('[INFO] Loaded Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "positive_encodings = []\n",
    "for i in range(50):\n",
    "    encoding = encoding_facenet.predict([pos_images[i:i+1]], batch_size = 1, verbose = 0)\n",
    "    positive_encodings.append(encoding)\n",
    "\n",
    "for i in range(50):\n",
    "    anchor_encoding = encoding_facenet.predict([anchor_images[i:i+1]], batch_size = 1, verbose = 0)\n",
    "    min_distance = 10000\n",
    "    min_index = -1\n",
    "    for j in range(len(positive_encodings)):\n",
    "        if i == j:\n",
    "            continue\n",
    "        distance = np.linalg.norm(anchor_encoding - positive_encodings[j])\n",
    "        print(distance)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            min_index = j\n",
    "    \n",
    "    pos_distance = np.linalg.norm(anchor_encoding - positive_encodings[i])\n",
    "    if (pos_distance <= min_distance):\n",
    "        print(str(i) + \" postive distance   : \" + str(pos_distance))\n",
    "        print(str(min_index) + \" negative distance : \" + str(min_distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(positive_encodings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
