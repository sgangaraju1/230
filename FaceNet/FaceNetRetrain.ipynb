{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "engaging-participant",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import scipy\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "from matplotlib   import pyplot as plt\n",
    "\n",
    "import tensorflow as     tf\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import Input\n",
    "from PIL          import Image\n",
    "\n",
    "from sklearn.metrics       import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.neighbors     import NearestNeighbors\n",
    "\n",
    "import boto3\n",
    "import zipfile\n",
    "\n",
    "import attribute_utils\n",
    "import dataset_utils\n",
    "import prediction_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "handmade-wholesale",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_MODEL = False\n",
    "\n",
    "S3_BUCKET_NAME = \"cs230project\"\n",
    "INPUTS_FNAME   = \"lfw_datasets_and_models.zip\"\n",
    "\n",
    "PATH_INPUTS_FNAME     = \"../../lfw_datasets_and_models.zip\" \n",
    "PATH_INPUTS           = \"../../lfw_datasets_and_models\"\n",
    "\n",
    "PATH_MODEL_FACENET_PRE = PATH_INPUTS + \"/models/facenet/pretrained/model/facenet_keras.h5\"\n",
    "\n",
    "PATH_MODEL_FACENET_TRAINED = PATH_INPUTS + \"/models/facenet/retrained/facenet_keras_trained.h5\"\n",
    "\n",
    "PATH_MODEL_FACENET_RET_ARCH    = PATH_INPUTS + \"/models/facenet/retrained/encoding_facenet_arch.json\"\n",
    "PATH_MODEL_FACENET_RET_WEIGHTS = PATH_INPUTS + \"/models/facenet/retrained/encoding_facenet_weights.h5\"\n",
    "\n",
    "PATH_MODEL_RESNET_ARCH    = PATH_INPUTS + \"/models/resnet/encoding_resnet_arch.json\"\n",
    "PATH_MODEL_RESNET_WEIGHTS = PATH_INPUTS + \"/models/resnet/encoding_resnet_weights.h5\"\n",
    "\n",
    "PATH_DATASET_BASE     = PATH_INPUTS + \"/datasets/\"\n",
    "\n",
    "PATH_DATASET_BASE_MASKED   = PATH_DATASET_BASE + \"/masked\"\n",
    "PATH_DATASET_BASE_UNMASKED = PATH_DATASET_BASE + \"/unmasked\"\n",
    "\n",
    "PATH_DATASET_MASKED_TRAIN = PATH_DATASET_BASE_MASKED + \"/train/\"\n",
    "PATH_DATASET_MASKED_VAL   = PATH_DATASET_BASE_MASKED + \"/validation/\"\n",
    "PATH_DATASET_MASKED_TEST  = PATH_DATASET_BASE_MASKED + \"/test/\"\n",
    "\n",
    "PATH_DATASET_UNMASKED_TRAIN = PATH_DATASET_BASE_UNMASKED + \"/train/\"\n",
    "PATH_DATASET_UNMASKED_VAL   = PATH_DATASET_BASE_UNMASKED + \"/validation/\"\n",
    "PATH_DATASET_UNMASKED_TEST  = PATH_DATASET_BASE_UNMASKED + \"/test/\"\n",
    "\n",
    "PATH_DATASET_ATTRIBUTES = PATH_DATASET_BASE + \"/attributes/lfw_attributes.csv\"\n",
    "\n",
    "PREDICT_THRESHOLD = 0.78125\n",
    "\n",
    "MODEL = \"facenet-pretrained\"\n",
    "# MODEL = \"facenet-retrained\"\n",
    "# MODEL = \"resnet\"\n",
    "\n",
    "# REQUIRED_SIZE = (224, 224)\n",
    "REQUIRED_SIZE = (160, 160)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ideal-talent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha = 0.5):\n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    pos_dist   = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis=-1)\n",
    "    neg_dist   = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis=-1)\n",
    "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), alpha)\n",
    "    loss       = tf.reduce_sum(tf.maximum(basic_loss, 0.0))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "processed-eligibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def batch_generator(batch_size=1):\n",
    "#     y_val     = np.zeros((batch_size, 2, 1))\n",
    "#     anchors   = np.zeros((batch_size, input_shape[0], input_shape[1], input_shape[2]))\n",
    "#     positives = np.zeros((batch_size, input_shape[0], input_shape[1], input_shape[2]))\n",
    "#     negatives = np.zeros((batch_size, input_shape[0], input_shape[1], input_shape[2]))\n",
    "    \n",
    "#     anchors[0]   = image_a\n",
    "#     positives[0] = image_p\n",
    "#     negatives[0] = image_n\n",
    "    \n",
    "#     x_data = {\n",
    "#         'anchor'         : anchors   ,\n",
    "#         'anchorPositive' : positives ,\n",
    "#         'anchorNegative' : negatives\n",
    "#     }\n",
    "    \n",
    "#     yield (x_data, [y_val, y_val, y_val])\n",
    "    \n",
    "# #     while True:\n",
    "# #         for i in range(batch_size):\n",
    "# #             positiveFace = faces[np.random.randint(len(faces))]\n",
    "# #             negativeFace = faces[np.random.randint(len(faces))]\n",
    "# #             while positiveFace == negativeFace:\n",
    "# #                 negativeFace = faces[np.random.randint(len(faces))]\n",
    "\n",
    "# #             positives[i] = images[positiveFace][np.random.randint(len(images[positiveFace]))]\n",
    "# #             anchors[i]   = images[positiveFace][np.random.randint(len(images[positiveFace]))]\n",
    "# #             negatives[i] = images[negativeFace][np.random.randint(len(images[negativeFace]))]\n",
    "        \n",
    "# #         x_data = {'anchor': anchors,\n",
    "# #                   'anchorPositive': positives,\n",
    "# #                   'anchorNegative': negatives\n",
    "# #                   }\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "smaller-canberra",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_arch_weights(path_model_arch, path_model_weights):\n",
    "    with open(path_model_arch, \"r\") as json_file:\n",
    "        model_arch = json_file.read()\n",
    "\n",
    "    #load the model architecture \n",
    "    model = model_from_json(model_arch)\n",
    "\n",
    "    #load the model weights\n",
    "    model.load_weights(path_model_weights)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "satellite-wildlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_():\n",
    "    ret = None\n",
    "    if (MODEL == \"facenet-pretrained\"):\n",
    "        print(\"[INFO] Loading Facenet pre-trained model...\", PATH_MODEL_FACENET_PRE)\n",
    "        ret = load_model(PATH_MODEL_FACENET_PRE)\n",
    "        print('[INFO] Loaded Model')\n",
    "    elif (MODEL == \"facenet-retrained\"):\n",
    "        ret = load_model_arch_weights(PATH_MODEL_FACENET_RET_ARCH, PATH_MODEL_FACENET_RET_WEIGHTS)\n",
    "    elif (MODEL == \"resnet\"):\n",
    "        ret = load_model_arch_weights(PATH_MODEL_RESNET_ARCH, PATH_MODEL_RESNET_WEIGHTS)\n",
    "\n",
    "    if (SHOW_MODEL):\n",
    "        ret.summary()\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "armed-isaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Given a FaceNet model and an image\n",
    "\"\"\"\n",
    "def get_embedding(model, face):\n",
    "    # scale pixel values\n",
    "    face = face.astype('float32')\n",
    "    # standardization\n",
    "    mean, std = face.mean(), face.std()\n",
    "    face = (face-mean)/std\n",
    "    # transfer face into one sample (3 dimension to 4 dimension)\n",
    "    sample = np.expand_dims(face, axis=0)\n",
    "    # make prediction to get embedding\n",
    "    yhat = model.predict(sample)\n",
    "    return yhat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "north-judge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading Facenet pre-trained model... ../../lfw_datasets_and_models/models/facenet/pretrained/model/facenet_keras.h5\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "[INFO] Loaded Model\n"
     ]
    }
   ],
   "source": [
    "facenet_model = load_model_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "level-married",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYERS_TO_FREEZE = 60\n",
    "for layer in facenet_model.layers[0: LAYERS_TO_FREEZE]:\n",
    "    layer.trainable  =  False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "raised-mainland",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=(160, 160, 3)\n",
    "A = Input(shape=input_shape, name = 'anchor')\n",
    "P = Input(shape=input_shape, name = 'anchorPositive')\n",
    "N = Input(shape=input_shape, name = 'anchorNegative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "disciplinary-electricity",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_A = facenet_model(A)\n",
    "enc_P = facenet_model(P)\n",
    "enc_N = facenet_model(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "indonesian-parker",
   "metadata": {},
   "outputs": [],
   "source": [
    "tripletModel = Model([A, P, N], [enc_A, enc_P, enc_N])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "stone-tribune",
   "metadata": {},
   "outputs": [],
   "source": [
    "tripletModel.compile(optimizer = 'adam', loss = triplet_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "processed-equipment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_a = dataset_utils.extract_face(PATH_DATASET_MASKED_TRAIN + \"/Aaron_Peirsol/Aaron_Peirsol_0001.jpg\")\n",
    "# image_p = dataset_utils.extract_face(PATH_DATASET_MASKED_TRAIN + \"/Aaron_Peirsol/Aaron_Peirsol_0002.jpg\")\n",
    "# image_n = dataset_utils.extract_face(PATH_DATASET_MASKED_TRAIN + \"/Zydrunas_Ilgauskas/Zydrunas_Ilgauskas_0001.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "stable-windows",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen = batch_generator(batch_size=16)\n",
    "# tripletModel.fit_generator(gen, epochs=1, steps_per_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "utility-handy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb_a = get_embedding(facenet_model, image_a)\n",
    "# emb_p = get_embedding(facenet_model, image_p)\n",
    "# emb_n = get_embedding(facenet_model, image_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fourth-consortium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diff_a_p = tf.reduce_sum(tf.square(tf.subtract(emb_a, emb_p)), axis=-1)\n",
    "# diff_a_n = tf.reduce_sum(tf.square(tf.subtract(emb_a, emb_n)), axis=-1)\n",
    "# diff_p_n = tf.reduce_sum(tf.square(tf.subtract(emb_p, emb_n)), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "faced-following",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(diff_a_p.numpy(), diff_a_n.numpy(), diff_p_n.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "sporting-october",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Loading dataset... ../../lfw_datasets_and_models/datasets//masked/train/\n",
      "i=0; subdir=Carl_Levin\n",
      "i=500; subdir=Craig_Morgan\n",
      "i=1000; subdir=Lawrence_Vito\n",
      "i=1500; subdir=Mary_Katherine_Smart\n",
      "i=2000; subdir=Bryan_Chui\n",
      "i=2500; subdir=Luis_Gonzalez\n",
      "i=3000; subdir=Ian_Wilmut\n",
      "i=3500; subdir=Melvin_Talbert\n",
      "i=4000; subdir=Ben_Kingsley\n",
      "i=4500; subdir=Claudia_Pechstein\n",
      "i=5000; subdir=Charmaine_Crooks\n",
      "i=5500; subdir=Roberto_Laratro\n"
     ]
    }
   ],
   "source": [
    "train_masked_x, train_masked_y = dataset_utils.load_dataset(PATH_DATASET_MASKED_TRAIN, REQUIRED_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "corresponding-feedback",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Loading dataset... ../../lfw_datasets_and_models/datasets//masked/validation/\n",
      "i=0; subdir=Mohamed_ElBaradei\n",
      "i=500; subdir=Torri_Edwards\n",
      "i=1000; subdir=Tamara_Brooks\n",
      "i=1500; subdir=Jean_Chretien\n"
     ]
    }
   ],
   "source": [
    "val_masked_x, val_masked_y = dataset_utils.load_dataset(PATH_DATASET_MASKED_VAL, REQUIRED_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "banned-membrane",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12223, 160, 160, 3) (12223,)\n"
     ]
    }
   ],
   "source": [
    "train_x = np.concatenate((train_masked_x, val_masked_x), axis=0)\n",
    "train_y = np.concatenate((train_masked_y, val_masked_y), axis=0)\n",
    "\n",
    "print(train_x.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "violent-crisis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Label encoding...\n",
      "Dataset: train=12223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 757, 4485, 4261, ..., 2902, 5078, 1465])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"[INFO] Label encoding...\")\n",
    "print(\"Dataset: train=%d\" % (train_x.shape[0]))\n",
    "\n",
    "# label encode targets\n",
    "out_encoder = LabelEncoder()\n",
    "encoder_arr = np.append (train_y, ['Ben_Affleck', 'Ben_Curtis', 'Ben_Howland', 'Benazir_Bhutto', 'Benjamin_Netanyahu', 'Bernard_Landry', 'Bernard_Law', 'Bertie_Ahern'])\n",
    "out_encoder.fit(encoder_arr)\n",
    "\n",
    "train_enc_y = out_encoder.transform(train_y)\n",
    "train_enc_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "precious-contractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Given a FaceNet model and an image\n",
    "\"\"\"\n",
    "def get_embedding(model, face):\n",
    "    # scale pixel values\n",
    "    face = face.astype('float32')\n",
    "    # standardization\n",
    "    mean, std = face.mean(), face.std()\n",
    "    face = (face-mean)/std\n",
    "    # transfer face into one sample (3 dimension to 4 dimension)\n",
    "    sample = np.expand_dims(face, axis=0)\n",
    "    # make prediction to get embedding\n",
    "    yhat = model.predict(sample)\n",
    "    return yhat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "spread-houston",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(dataset, facenet_model):\n",
    "    ret = list()\n",
    "    i = 0\n",
    "    for face in dataset:\n",
    "        emd = get_embedding(facenet_model, face)\n",
    "        ret.append(emd)\n",
    "        if (i % 1000 == 0):\n",
    "            print(i)\n",
    "        i += 1\n",
    "    ret = np.asarray(ret)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "portable-audience",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build a dataframe of Person labels with the counts of images in the dataset for that person.\n",
    "# df_train_enc_y        = pd.DataFrame(train_enc_y)\n",
    "# df_train_enc_y        = df_train_enc_y.rename(columns = {0: \"Person\"})\n",
    "# df_train_enc_counts_y = df_train_enc_y.groupby(\"Person\").size().reset_index()\n",
    "# df_train_enc_counts_y = df_train_enc_counts_y.rename(columns = {0: \"Count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "artificial-nursing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_enc_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "instructional-queensland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_enc_counts_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "retained-myrtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(batch_size=16):\n",
    "    y_val     = np.zeros((batch_size, 2, 1))\n",
    "    anchors   = np.zeros((batch_size, input_shape[0], input_shape[1], input_shape[2]))\n",
    "    positives = np.zeros((batch_size, input_shape[0], input_shape[1], input_shape[2]))\n",
    "    negatives = np.zeros((batch_size, input_shape[0], input_shape[1], input_shape[2]))\n",
    "    \n",
    "    while True:\n",
    "        for i in range(batch_size):\n",
    "            idx_anchor = np.random.randint(len(train_x[0:10]))\n",
    "            idx_prev   = idx_anchor - 1\n",
    "            idx_next   = idx_anchor + 1\n",
    "            prs_anchor = train_enc_y[idx_anchor]\n",
    "\n",
    "            if (idx_prev < 0):\n",
    "                idx_prev = idx_anchor                \n",
    "\n",
    "            if (idx_next >= len(train_x[0:10])):\n",
    "                idx_next = idx_anchor\n",
    "\n",
    "            if (train_enc_y[idx_prev] != prs_anchor):\n",
    "                idx_prev = idx_anchor\n",
    "\n",
    "            if (train_enc_y[idx_next] != prs_anchor):\n",
    "                idx_next = idx_anchor\n",
    "\n",
    "            idx_positive = idx_prev\n",
    "            if (np.random.random() > 0.5):\n",
    "                idx_positive = idx_next\n",
    "\n",
    "            idx_negative = np.random.randint(len(train_x[0:10]))\n",
    "            while ((idx_negative == idx_anchor) or (idx_negative == idx_positive)):\n",
    "                idx_negative = np.random.randint(len(train_x[0:10]))\n",
    "\n",
    "            if ((idx_anchor == 3) or (idx_anchor == 4)):\n",
    "                print(idx_anchor, idx_positive, idx_negative, idx_prev, idx_next)\n",
    "                \n",
    "            anchors[i]   = train_x[idx_anchor]\n",
    "            positives[i] = train_x[idx_positive]\n",
    "            negatives[i] = train_x[idx_negative]\n",
    "        \n",
    "        x_data = {\n",
    "            'anchor'         : anchors,\n",
    "            'anchorPositive' : positives,\n",
    "            'anchorNegative' : negatives\n",
    "        }\n",
    "\n",
    "        yield (x_data, [y_val, y_val, y_val])        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "solved-monitoring",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(batch_size=16):\n",
    "    y_val     = np.zeros((batch_size, 2, 1))\n",
    "    anchors   = np.zeros((batch_size, input_shape[0], input_shape[1], input_shape[2]))\n",
    "    positives = np.zeros((batch_size, input_shape[0], input_shape[1], input_shape[2]))\n",
    "    negatives = np.zeros((batch_size, input_shape[0], input_shape[1], input_shape[2]))\n",
    "    \n",
    "    while True:\n",
    "        for i in range(batch_size):\n",
    "            anchors[i]   = train_x[3]\n",
    "            positives[i] = train_x[4]\n",
    "            negatives[i] = train_x[1]\n",
    "\n",
    "        x_data = {\n",
    "            'anchor'         : anchors,\n",
    "            'anchorPositive' : positives,\n",
    "            'anchorNegative' : negatives\n",
    "        }\n",
    "\n",
    "        yield (x_data, [y_val, y_val, y_val])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "spiritual-convergence",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 25s 25s/step - loss: 493.0129 - inception_resnet_v1_loss: 172.7406 - inception_resnet_v1_1_loss: 30.2065 - inception_resnet_v1_2_loss: 290.0658\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa3c677c6a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = batch_generator(batch_size=16)\n",
    "tripletModel.fit_generator(gen, epochs=1, steps_per_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "rough-heath",
   "metadata": {},
   "outputs": [],
   "source": [
    "facenet_model.save(PATH_MODEL_FACENET_TRAINED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "incomplete-nepal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "train_emd_x = get_embeddings(train_x[:50], facenet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "banner-movie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize input vectors\n",
    "in_encoder       = Normalizer(norm='l2')\n",
    "train_emd_norm_x = in_encoder.transform(train_emd_x)\n",
    "# val_emd_norm_x   = in_encoder.transform(val_emd_x)\n",
    "# test_emd_norm_x  = in_encoder.transform(test_emd_x)\n",
    "\n",
    "# train_emd_norm_x = train_emd_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "intermediate-saying",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0575176\n",
      "1.6898618\n",
      "1.3890479\n"
     ]
    }
   ],
   "source": [
    "diff = tf.reduce_sum(tf.square(tf.subtract(train_emd_norm_x[3], train_emd_norm_x[4])), axis=-1)\n",
    "print(diff.numpy())\n",
    "\n",
    "diff = tf.reduce_sum(tf.square(tf.subtract(train_emd_norm_x[3], train_emd_norm_x[1])), axis=-1)\n",
    "print(diff.numpy())\n",
    "\n",
    "diff = tf.reduce_sum(tf.square(tf.subtract(train_emd_norm_x[1], train_emd_norm_x[4])), axis=-1)\n",
    "print(diff.numpy())\n",
    "\n",
    "# diff = tf.reduce_sum(tf.square(tf.subtract(train_emd_norm_x[0], train_emd_norm_x[1])), axis=-1)\n",
    "# print(diff.numpy())\n",
    "\n",
    "# diff = tf.reduce_sum(tf.square(tf.subtract(train_emd_norm_x[1], train_emd_norm_x[2])), axis=-1)\n",
    "# print(diff.numpy())\n",
    "\n",
    "# diff = tf.reduce_sum(tf.square(tf.subtract(train_emd_norm_x[2], train_emd_norm_x[3])), axis=-1)\n",
    "# print(diff.numpy())\n",
    "\n",
    "# diff = tf.reduce_sum(tf.square(tf.subtract(train_emd_norm_x[2], train_emd_norm_x[3])), axis=-1)\n",
    "# print(diff.numpy())\n",
    "\n",
    "# diff = tf.reduce_sum(tf.square(tf.subtract(train_emd_norm_x[24], train_emd_norm_x[25])), axis=-1)\n",
    "# print(diff.numpy())\n",
    "\n",
    "# diff = tf.reduce_sum(tf.square(tf.subtract(train_emd_norm_x[25], train_emd_norm_x[26])), axis=-1)\n",
    "# print(diff.numpy())\n",
    "\n",
    "# diff = tf.reduce_sum(tf.square(tf.subtract(train_emd_norm_x[27], train_emd_norm_x[28])), axis=-1)\n",
    "# print(diff.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-killing",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
